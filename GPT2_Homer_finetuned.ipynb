{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GPT2-Homer-finetuned.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rstGOiprFMQP",
    "outputId": "152df88f-ba0f-489e-9819-d622d948bbdf"
   },
   "source": [
    "!pip install transformers==4.2.2\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.2.2\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 14.7MB/s \n",
      "\u001B[?25hCollecting sacremoses\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 48.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2019.12.20)\n",
      "Collecting tokenizers==0.9.4\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 48.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.10.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.2) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.2) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.2) (3.4.1)\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.2\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rY3tHnLDJvl6"
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nikokons/gpt2-greek\")\n",
    "\n",
    "train_path = '/content/iliadaodysseia.txt'\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pEkyWQTKKsGx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c993d8ff-fbdb-4cf9-8db2-b38bed17053a"
   },
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,data_collator\n",
    "\n",
    "train_dataset,data_collator = load_dataset(train_path,tokenizer)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614218 > 1024). Running this sequence through the model will result in indexing errors\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7PITqSxLFeI",
    "outputId": "533cf2c4-27a3-405a-c2dc-0258a0677487"
   },
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"nikokons/gpt2-greek\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-homer\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "\n",
    "   \n",
    ")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "pEE2F1_cLTX4",
    "outputId": "78428a2d-773d-40b1-8bf0-ef84ac233ea8"
   },
   "source": [
    "trainer.train()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 03:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=4.489407552083334, metrics={'train_runtime': 221.4331, 'train_samples_per_second': 2.032, 'total_flos': 1131384606031872, 'epoch': 3.0})"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_WtZNzohLY_c"
   },
   "source": [
    "trainer.save_model()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QHW4-yU7L40E"
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation',model='./gpt2-homer', tokenizer='nikokons/gpt2-greek')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhasdSGBMK8p",
    "outputId": "21108fb8-f1f4-4461-b7bb-dc954471429d"
   },
   "source": [
    "result = generator('ÎšÎ±Î¹ Ï„ÏŒÏ„Îµ Î¿ ÎŸÎ´Ï…ÏƒÏƒÎ­Î±Ï‚ ',\n",
    "                   do_sample=True, \n",
    "    max_length=200, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    "                    )[0]['generated_text']\n",
    "# print(result)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21999 for open-end generation.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTQywEgVWsrm",
    "outputId": "5a013f7e-dfec-4540-a39e-34ed42b19d8e"
   },
   "source": [
    "print(result)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "ÎšÎ±Î¹ Ï„ÏŒÏ„Îµ Î¿ ÎŸÎ´Ï…ÏƒÏƒÎ­Î±Ï‚  Ï„Î¿Ï… Î±Ï€Î·Î»Î¿Î³Î®Î¸Î· ÎºÎ¹ ÎµÎ¯Ï€Îµ:\n",
      "Â«ÎšÎ±Î¹ Ï„ÏÏÎ±, Ï€Î·Î³Î±Î¯Î½Î¿Î½Ï„Î±Ï‚ ÏƒÏ„Î¿Ï…Ï‚ Î¸ÎµÎ¿ÏÏ‚ Ï„' Î±ÏÎ½Î¹Î¬ Ï„ÏÎ±Î½Î® ÏƒÎ¿Ï… Ï€Î¹Î± Î²Î¿Ï…Î»Î® Î´ÎµÎ½ 'ÏÎ¸Îµ,\n",
      "Ï„Î¿ ÏÎ¯Î¾ÎµÎ¹ Î¿ Î¬Î»Î»Î¿Ï‚, Î¿ Î•Ï…ÏÏÎ»Î¿Ï‡Î¿Ï‚, Î¿ ÎœÎµÎ½Î­Î»Î±Î¿Ï‚ ÂµÎµ Ï„Î¿ ÏƒÏ€Î®Î»Î¹Î¿,\n",
      "Î³Î¹Î±Ï„Î¯ Î±Ï€' Ï„Î¹Ï‚ Ï€Î¹Î¿Ï„ÏƒÎ¹Î¬Î´ÎµÏ‚ Î±Ï€Î±ÏÎ¬Î»Î»Î±Ï‡Ï„ÎµÏ‚ Ï„Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï€Î¹Î± Ï„ÏŒÏƒÎ¿ Ï€Ï‰Ï‚ Î´ÎµÎ½ ÏƒÎ¿Ï… Î´Î¹ÏÏ‡ÎµÎ¹Ï‚.\n",
      "ÎšÎ¹ Î±Ï…Ï„Î¿Î¯ Ï„Î± Ï‡Ï‰ÏÎ¬Ï†Î¹Î± ÎºÎ±Î¹ Ï„Î± Î½ÎµÏÎ¬ Ï„Ï‰Î½ ÏƒÎºÎ»Î¬Î²Ï‰Î½ Î¾ÎµÎºÎ¬Î½Î¿Ï…Î½\n",
      "ÎºÎ±Î¹ ÏƒÎºÏŒÏÏ€Î¹ÏƒÎ±Î½ ÏƒÏ„Î¿ Î Î±ÏÏ€Î·ÏƒÏƒÏŒ ÏƒÏ„Î· Î³Î· Ï„Î¿Î½ Ï€ÏŒÎ»ÎµÂµÎ¿, ÎºÎ¹ Î¿ ÎœÎµÎ½Î­Î»Î±Î¿Ï‚ Ï„Î± Ï†Î±Î³Î¹Î¬ ÎºÎ±Î¹ Ï„Î¿ Î¾Î¬ÏÏ€Î¹Î¿\n",
      "ÏƒÎ±Î½ Ï„Î± ÏƒÏ€Î±Î¸Î¹Î¬ Ï„Î¿Ï…Ï‚' Ï„Î¿ ÏˆÎ·Î»ÏŒ ÎºÏÎ±ÏƒÎ¯ Ï‡Ï„Ï…Ï€Î¹Î¿ÏÎ½Ï„Î±Î¹ ÎºÎ±Î¹ Ï„Î± ÎºÏÎ¿ÏƒÏƒÏ‰ÂµÎ­Î½Î· ÏˆÏ…Ï‡Î±Î½Î¸Î­Ï‚,\n",
      "ÎºÎ±Î¹ Ï„Î± Ï€Î¿Î»Î»Î¬ ÏƒÎµ ÎºÎ¬Î¸Î¹Î¶Î±Î½ ÏƒÏ„Î¿ Î±Î½Î¬ÂµÎµÏÎ¿ Î¿Ï‡Ï„ÏÎµÏÎµÎ¹;\n",
      "\n",
      "Î Î¿ Î³Î¹ÎµÏƒÏŒÏ‚ Ï„Î¿Ï…Ï‚ Ï€Î¿Î»Î»Î¿ÏÏ‚ Î´Î¯Î½ÎµÎ¹' ÎºÎ¹\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyZLPIyqXR0i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Output generated:\n",
    "\n",
    "ÎšÎ±Î¹ Ï„ÏŒÏ„Îµ Î¿ ÎŸÎ´Ï…ÏƒÏƒÎ­Î±Ï‚  Ï„Î¿Ï… Î±Ï€Î·Î»Î¿Î³Î®Î¸Î· ÎºÎ¹ ÎµÎ¯Ï€Îµ:\n",
    "Â«ÎšÎ±Î¹ Ï„ÏÏÎ±, Ï€Î·Î³Î±Î¯Î½Î¿Î½Ï„Î±Ï‚ ÏƒÏ„Î¿Ï…Ï‚ Î¸ÎµÎ¿ÏÏ‚ Ï„' Î±ÏÎ½Î¹Î¬ Ï„ÏÎ±Î½Î® ÏƒÎ¿Ï… Ï€Î¹Î± Î²Î¿Ï…Î»Î® Î´ÎµÎ½ 'ÏÎ¸Îµ,\n",
    "Ï„Î¿ ÏÎ¯Î¾ÎµÎ¹ Î¿ Î¬Î»Î»Î¿Ï‚, Î¿ Î•Ï…ÏÏÎ»Î¿Ï‡Î¿Ï‚, Î¿ ÎœÎµÎ½Î­Î»Î±Î¿Ï‚ ÂµÎµ Ï„Î¿ ÏƒÏ€Î®Î»Î¹Î¿,\n",
    "Î³Î¹Î±Ï„Î¯ Î±Ï€' Ï„Î¹Ï‚ Ï€Î¹Î¿Ï„ÏƒÎ¹Î¬Î´ÎµÏ‚ Î±Ï€Î±ÏÎ¬Î»Î»Î±Ï‡Ï„ÎµÏ‚ Ï„Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï€Î¹Î± Ï„ÏŒÏƒÎ¿ Ï€Ï‰Ï‚ Î´ÎµÎ½ ÏƒÎ¿Ï… Î´Î¹ÏÏ‡ÎµÎ¹Ï‚.\n",
    "ÎšÎ¹ Î±Ï…Ï„Î¿Î¯ Ï„Î± Ï‡Ï‰ÏÎ¬Ï†Î¹Î± ÎºÎ±Î¹ Ï„Î± Î½ÎµÏÎ¬ Ï„Ï‰Î½ ÏƒÎºÎ»Î¬Î²Ï‰Î½ Î¾ÎµÎºÎ¬Î½Î¿Ï…Î½\n",
    "ÎºÎ±Î¹ ÏƒÎºÏŒÏÏ€Î¹ÏƒÎ±Î½ ÏƒÏ„Î¿ Î Î±ÏÏ€Î·ÏƒÏƒÏŒ ÏƒÏ„Î· Î³Î· Ï„Î¿Î½ Ï€ÏŒÎ»ÎµÂµÎ¿, ÎºÎ¹ Î¿ ÎœÎµÎ½Î­Î»Î±Î¿Ï‚ Ï„Î± Ï†Î±Î³Î¹Î¬ ÎºÎ±Î¹ Ï„Î¿ Î¾Î¬ÏÏ€Î¹Î¿\n",
    "ÏƒÎ±Î½ Ï„Î± ÏƒÏ€Î±Î¸Î¹Î¬ Ï„Î¿Ï…Ï‚' Ï„Î¿ ÏˆÎ·Î»ÏŒ ÎºÏÎ±ÏƒÎ¯ Ï‡Ï„Ï…Ï€Î¹Î¿ÏÎ½Ï„Î±Î¹ ÎºÎ±Î¹ Ï„Î± ÎºÏÎ¿ÏƒÏƒÏ‰ÂµÎ­Î½Î· ÏˆÏ…Ï‡Î±Î½Î¸Î­Ï‚,\n",
    "ÎºÎ±Î¹ Ï„Î± Ï€Î¿Î»Î»Î¬ ÏƒÎµ ÎºÎ¬Î¸Î¹Î¶Î±Î½ ÏƒÏ„Î¿ Î±Î½Î¬ÂµÎµÏÎ¿ Î¿Ï‡Ï„ÏÎµÏÎµÎ¹;\n",
    "\n",
    "Î Î¿ Î³Î¹ÎµÏƒÏŒÏ‚ Ï„Î¿Ï…Ï‚ Ï€Î¿Î»Î»Î¿ÏÏ‚ Î´Î¯Î½ÎµÎ¹' ÎºÎ¹\n",
    "\n",
    "\n",
    "The output can be improved, while it is syntactically not bad , some of the word sequences are not meaningful\n"
   ]
  }
 ]
}