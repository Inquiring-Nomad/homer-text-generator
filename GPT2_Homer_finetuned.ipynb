{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GPT2-Homer-finetuned.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rstGOiprFMQP",
    "outputId": "152df88f-ba0f-489e-9819-d622d948bbdf"
   },
   "source": [
    "!pip install transformers==4.2.2\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.2.2\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
      "\u001B[K     |████████████████████████████████| 1.8MB 14.7MB/s \n",
      "\u001B[?25hCollecting sacremoses\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001B[K     |████████████████████████████████| 901kB 48.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2019.12.20)\n",
      "Collecting tokenizers==0.9.4\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001B[K     |████████████████████████████████| 2.9MB 48.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.10.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.2) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.2) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.2) (3.4.1)\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.2\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rY3tHnLDJvl6"
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nikokons/gpt2-greek\")\n",
    "\n",
    "train_path = '/content/iliadaodysseia.txt'\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pEkyWQTKKsGx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c993d8ff-fbdb-4cf9-8db2-b38bed17053a"
   },
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,data_collator\n",
    "\n",
    "train_dataset,data_collator = load_dataset(train_path,tokenizer)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614218 > 1024). Running this sequence through the model will result in indexing errors\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7PITqSxLFeI",
    "outputId": "533cf2c4-27a3-405a-c2dc-0258a0677487"
   },
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"nikokons/gpt2-greek\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-homer\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "\n",
    "   \n",
    ")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "pEE2F1_cLTX4",
    "outputId": "78428a2d-773d-40b1-8bf0-ef84ac233ea8"
   },
   "source": [
    "trainer.train()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 03:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=4.489407552083334, metrics={'train_runtime': 221.4331, 'train_samples_per_second': 2.032, 'total_flos': 1131384606031872, 'epoch': 3.0})"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_WtZNzohLY_c"
   },
   "source": [
    "trainer.save_model()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QHW4-yU7L40E"
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation',model='./gpt2-homer', tokenizer='nikokons/gpt2-greek')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhasdSGBMK8p",
    "outputId": "21108fb8-f1f4-4461-b7bb-dc954471429d"
   },
   "source": [
    "result = generator('Και τότε ο Οδυσσέας ',\n",
    "                   do_sample=True, \n",
    "    max_length=200, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    "                    )[0]['generated_text']\n",
    "# print(result)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21999 for open-end generation.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTQywEgVWsrm",
    "outputId": "5a013f7e-dfec-4540-a39e-34ed42b19d8e"
   },
   "source": [
    "print(result)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Και τότε ο Οδυσσέας  του απηλογήθη κι είπε:\n",
      "«Και τώρα, πηγαίνοντας στους θεούς τ' αρνιά τρανή σου πια βουλή δεν 'ρθε,\n",
      "το ρίξει ο άλλος, ο Ευρύλοχος, ο Μενέλαος µε το σπήλιο,\n",
      "γιατί απ' τις πιοτσιάδες απαράλλαχτες του είναι πια τόσο πως δεν σου διώχεις.\n",
      "Κι αυτοί τα χωράφια και τα νερά των σκλάβων ξεκάνουν\n",
      "και σκόρπισαν στο Παρπησσό στη γη τον πόλεµο, κι ο Μενέλαος τα φαγιά και το ξάρπιο\n",
      "σαν τα σπαθιά τους' το ψηλό κρασί χτυπιούνται και τα κροσσωµένη ψυχανθές,\n",
      "και τα πολλά σε κάθιζαν στο ανάµερο οχτρεύει;\n",
      "\n",
      "Πο γιεσός τους πολλούς δίνει' κι\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyZLPIyqXR0i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Output generated:\n",
    "\n",
    "Και τότε ο Οδυσσέας  του απηλογήθη κι είπε:\n",
    "«Και τώρα, πηγαίνοντας στους θεούς τ' αρνιά τρανή σου πια βουλή δεν 'ρθε,\n",
    "το ρίξει ο άλλος, ο Ευρύλοχος, ο Μενέλαος µε το σπήλιο,\n",
    "γιατί απ' τις πιοτσιάδες απαράλλαχτες του είναι πια τόσο πως δεν σου διώχεις.\n",
    "Κι αυτοί τα χωράφια και τα νερά των σκλάβων ξεκάνουν\n",
    "και σκόρπισαν στο Παρπησσό στη γη τον πόλεµο, κι ο Μενέλαος τα φαγιά και το ξάρπιο\n",
    "σαν τα σπαθιά τους' το ψηλό κρασί χτυπιούνται και τα κροσσωµένη ψυχανθές,\n",
    "και τα πολλά σε κάθιζαν στο ανάµερο οχτρεύει;\n",
    "\n",
    "Πο γιεσός τους πολλούς δίνει' κι\n",
    "\n",
    "\n",
    "The output can be improved, while it is syntactically not bad , some of the word sequences are not meaningful\n"
   ]
  }
 ]
}